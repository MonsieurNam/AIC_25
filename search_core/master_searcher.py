# search_core/master_searcher.py

from collections import defaultdict
from typing import Dict, Any, Optional, List
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from tqdm import tqdm

# Import c√°c module c·ªët l√µi c·ªßa h·ªá th·ªëng
from search_core.basic_searcher import BasicSearcher
from search_core.semantic_searcher import SemanticSearcher
from search_core.trake_solver import TRAKESolver
from search_core.gemini_text_handler import GeminiTextHandler
from search_core.openai_handler import OpenAIHandler
from search_core.task_analyzer import TaskType
from search_core.mmr_builder import MMRResultBuilder 
from search_core.query_decomposer import QueryDecomposer

class MasterSearcher:
    """
    L·ªõp ƒëi·ªÅu ph·ªëi ch√≠nh c·ªßa h·ªá th·ªëng t√¨m ki·∫øm (Hybrid AI Edition).
    N√≥ qu·∫£n l√Ω v√† ƒëi·ªÅu ph·ªëi c√°c AI Handler kh√°c nhau (Gemini cho text, OpenAI cho vision)
    ƒë·ªÉ gi·∫£i quy·∫øt c√°c lo·∫°i truy v·∫•n ph·ª©c t·∫°p.
    """

    def __init__(self, 
                 basic_searcher: BasicSearcher, 
                 rerank_model,
                 gemini_api_key: Optional[str] = None,
                 openai_api_key: Optional[str] = None,
                 entities_path: str = None,
                 clip_features_path: str = None,
                 video_path_map: dict = None):
        """
        Kh·ªüi t·∫°o MasterSearcher v√† h·ªá sinh th√°i AI lai.
        """
        print("--- üß† Kh·ªüi t·∫°o Master Searcher (Hybrid AI Edition) ---")
        
        self.semantic_searcher = SemanticSearcher(basic_searcher=basic_searcher, rerank_model=rerank_model)
        self.mmr_builder: Optional[MMRResultBuilder] = None
        if clip_features_path and os.path.exists(clip_features_path):
            try:
                print(f"--- üöö ƒêang t·∫£i to√†n b·ªô CLIP features cho MMR t·ª´: {clip_features_path} ---")
                all_clip_features = np.load(clip_features_path)
                self.mmr_builder = MMRResultBuilder(clip_features=all_clip_features)
            except Exception as e:
                 print(f"--- ‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o MMR Builder: {e}. MMR s·∫Ω b·ªã v√¥ hi·ªáu h√≥a. ---")
        else:
            print("--- ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file CLIP features, MMR s·∫Ω kh√¥ng ho·∫°t ƒë·ªông. ---")
        self.video_path_map = video_path_map
        self.gemini_handler: Optional[GeminiTextHandler] = None
        self.openai_handler: Optional[OpenAIHandler] = None
        self.trake_solver: Optional[TRAKESolver] = None
        self.query_decomposer: Optional[QueryDecomposer] = None
        self.ai_enabled = False
        self.known_entities: set = set()
        print(f"--- ‚úÖ Master Searcher ƒë√£ s·∫µn s√†ng! (AI Enabled: {self.ai_enabled}) ---")
        
        if entities_path and os.path.exists(entities_path):
            try:
                print(f"--- üìö ƒêang t·∫£i T·ª´ ƒëi·ªÉn ƒê·ªëi t∆∞·ª£ng t·ª´: {entities_path} ---")
                with open(entities_path, 'r') as f:
                    entities_list = [entity.lower() for entity in json.load(f)]
                    self.known_entities = set(entities_list)
                print(f"--- ‚úÖ T·∫£i th√†nh c√¥ng {len(self.known_entities)} th·ª±c th·ªÉ ƒë√£ bi·∫øt. ---")
            except Exception as e:
                print(f"--- ‚ö†Ô∏è L·ªói khi t·∫£i T·ª´ ƒëi·ªÉn ƒê·ªëi t∆∞·ª£ng: {e}. Semantic Grounding s·∫Ω b·ªã v√¥ hi·ªáu h√≥a. ---")
                
        # --- Kh·ªüi t·∫°o v√† x√°c th·ª±c Gemini Handler cho c√°c t√°c v·ª• TEXT ---
        if gemini_api_key:
            try:
                self.gemini_handler = GeminiTextHandler(api_key=gemini_api_key)
                if self.known_entities and self.gemini_handler:
                    self.gemini_handler.load_known_entities(self.known_entities)
                self.query_decomposer = QueryDecomposer(self.gemini_handler)
                self.ai_enabled = True
            except Exception as e:
                print(f"--- ‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o Gemini Handler: {e}. C√°c t√≠nh nƒÉng text AI s·∫Ω b·ªã h·∫°n ch·∫ø. ---")

        # --- Kh·ªüi t·∫°o v√† x√°c th·ª±c OpenAI Handler cho c√°c t√°c v·ª• VISION ---
        if openai_api_key:
            try:
                self.openai_handler = OpenAIHandler(api_key=openai_api_key)
                if not self.openai_handler.check_api_health():
                    self.openai_handler = None
                else:
                    self.ai_enabled = True
            except Exception as e:
                print(f"--- ‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o OpenAI Handler: {e}. C√°c t√≠nh nƒÉng vision AI s·∫Ω b·ªã h·∫°n ch·∫ø. ---")
        
        # --- Kh·ªüi t·∫°o c√°c Solver ph·ª©c t·∫°p n·∫øu c√°c handler c·∫ßn thi·∫øt ƒë√£ s·∫µn s√†ng ---
        if self.gemini_handler:
            self.trake_solver = TRAKESolver(ai_handler=self.gemini_handler)

        print(f"--- ‚úÖ Master Searcher ƒë√£ s·∫µn s√†ng! (AI Enabled: {self.ai_enabled}) ---")
        
    def perform_semantic_grounding(self, entities_to_ground: List[str]) -> Dict[str, str]:
        """
        D·ªãch c√°c nh√£n entity t·ª± do v·ªÅ c√°c nh√£n chu·∫©n c√≥ trong t·ª´ ƒëi·ªÉn.
        Tr·∫£ v·ªÅ m·ªôt dictionary mapping: {entity_g·ªëc: entity_ƒë√£_d·ªãch}.
        """
        if not entities_to_ground or not self.known_entities_prompt_segment:
            return {}

        print(f"--- üß† B·∫Øt ƒë·∫ßu Semantic Grounding cho: {entities_to_ground} ---")
        
        prompt = (
            f"You are a helpful assistant. Your task is to map a list of input entities to the closest matching entities from a predefined dictionary. "
            f"For each input entity, find the single most appropriate term from the dictionary.\n\n"
            f"**Predefined Dictionary:**\n{self.known_entities_prompt_segment}\n\n"
            f"**Input Entities to Map:**\n{json.dumps(entities_to_ground)}\n\n"
            f"Provide your answer ONLY as a valid JSON object mapping each input entity to its corresponding dictionary term. "
            f"Example format: {{\"input_entity_1\": \"dictionary_term_1\", \"input_entity_2\": \"dictionary_term_2\"}}"
        )
        
        try:
            response = self.model.generate_content(prompt)
            # Gi·∫£ ƒë·ªãnh response.text l√† m·ªôt chu·ªói JSON h·ª£p l·ªá
            grounding_map = json.loads(response.text)
            print(f"    -> K·∫øt qu·∫£ Grounding: {grounding_map}")
            return grounding_map
        except Exception as e:
            print(f"--- ‚ö†Ô∏è L·ªói trong qu√° tr√¨nh Semantic Grounding: {e} ---")
            # Fallback: Tr·∫£ v·ªÅ mapping r·ªóng n·∫øu c√≥ l·ªói
            return {}
    
    def _deduplicate_temporally(self, results: List[Dict[str, Any]], time_threshold: int = 2) -> List[Dict[str, Any]]:
        """
        L·ªçc c√°c k·∫øt qu·∫£ b·ªã tr√πng l·∫∑p v·ªÅ m·∫∑t th·ªùi gian trong c√πng m·ªôt video.

        Args:
            results (List[Dict[str, Any]]): Danh s√°ch k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒëi·ªÉm.
            time_threshold (int): Ng∆∞·ª°ng th·ªùi gian (gi√¢y). C√°c frame trong c√πng video
                                  c√°ch nhau d∆∞·ªõi ng∆∞·ª°ng n√†y s·∫Ω b·ªã coi l√† tr√πng l·∫∑p.

        Returns:
            List[Dict[str, Any]]: Danh s√°ch k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l·ªçc.
        """
        if not results:
            return []

        print(f"--- üõ°Ô∏è B·∫Øt ƒë·∫ßu L·ªçc Tr√πng l·∫∑p Th·ªùi gian (Ng∆∞·ª°ng: {time_threshold}s)... ---")
        
        last_timestamp_per_video = {}
        
        deduplicated_results = []

        for result in results:
            video_id = result.get('video_id')
            timestamp = result.get('timestamp')

            if not video_id or timestamp is None:
                continue # B·ªè qua n·∫øu thi·∫øu th√¥ng tin

            last_seen_timestamp = last_timestamp_per_video.get(video_id)

            if last_seen_timestamp is None or abs(timestamp - last_seen_timestamp) > time_threshold:
                deduplicated_results.append(result)
                last_timestamp_per_video[video_id] = timestamp
        
        print(f"--- ‚úÖ L·ªçc ho√†n t·∫•t. T·ª´ {len(results)} -> c√≤n {len(deduplicated_results)} k·∫øt qu·∫£. ---")
        return deduplicated_results

# /search_core/master_searcher.py - PHI√äN B·∫¢N CU·ªêI C√ôNG (COPY V√Ä THAY TH·∫æ)

    def search(self, query: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Th·ª±c thi t√¨m ki·∫øm theo chi·∫øn l∆∞·ª£c "PHOENIX REBORN" ƒë√£ ƒë∆∞·ª£c tinh g·ªçn.
        Lu·ªìng x·ª≠ l√Ω duy nh·∫•t, kh√¥ng ph√¢n nh√°nh.
        """
        print("\n" + "="*20 + " üöÄ K√≠ch ho·∫°t Chi·∫øn d·ªãch PHOENIX REBORN " + "="*20)

        # === B∆Ø·ªöC 1: PH√ÇN R√É TRUY V·∫§N ===
        if not self.query_decomposer or not self.ai_enabled:
             print("--- ‚ö†Ô∏è Decomposer ch∆∞a s·∫µn s√†ng. Ch·∫°y ·ªü ch·∫ø ƒë·ªô KIS ƒë∆°n gi·∫£n. ---")
             sub_queries = [query]
        else:
             sub_queries = self.query_decomposer.decompose(query)
        
        print(f"   -> Truy v·∫•n ƒë∆∞·ª£c ph√¢n r√£ th√†nh {len(sub_queries)} truy v·∫•n con: {sub_queries}")

        if not sub_queries:
            return {"task_type": TaskType.KIS, "results": [], "query_analysis": {}}

        # === B∆Ø·ªöC 2: N√âM L∆Ø·ªöI SONG SONG ===
        kis_retrieval_count = int(config.get('initial_retrieval_slider', 500))
        weights = {
            'w_clip': config.get('w_clip_slider', 0.4),
            'w_obj': config.get('w_obj_slider', 0.3),
            'w_semantic': config.get('w_semantic_slider', 0.3),
            'w_spatial': config.get('w_spatial_slider', 0.25),
            'w_fine_grained': config.get('w_fine_grained_slider', 0.25)
        }
        all_results_raw = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_query = {
                executor.submit(
                    self.semantic_searcher.search, query_text=sq,
                    top_k_final=kis_retrieval_count, top_k_retrieval=kis_retrieval_count,
                    precomputed_analysis={}, weights=weights
                ): sq for sq in sub_queries
            }
            for future in tqdm(as_completed(future_to_query), total=len(sub_queries), desc="   -> ƒêang N√©m L∆∞·ªõi"):
                sub_query = future_to_query[future]
                try:
                    results = future.result()
                    for res in results: res['matched_query'] = sub_query
                    all_results_raw.extend(results)
                except Exception as exc:
                    print(f"   -> ‚ùå L·ªói khi t√¨m ki·∫øm cho sub-query '{sub_query}': {exc}")

        print(f"   -> Thu v·ªÅ t·ªïng c·ªông {len(all_results_raw)} ·ª©ng vi√™n th√¥.")
        if not all_results_raw:
             return {"task_type": TaskType.KIS, "results": [], "query_analysis": {"sub_queries": sub_queries}}

        # === B∆Ø·ªöC 3: H·ª¢P NH·∫§T V√Ä T√çNH ƒêI·ªÇM ƒê·ªíNG XU·∫§T HI·ªÜN ===
        print("   -> ƒêang h·ª£p nh·∫•t k·∫øt qu·∫£ v√† t√≠nh ƒëi·ªÉm theo b·∫±ng ch·ª©ng...")
        merged_candidates = defaultdict(lambda: {'sum_score': 0.0, 'matched_queries': set(), 'data': None})
        for res in all_results_raw:
            key = res['keyframe_id']
            merged_candidates[key]['sum_score'] += res.get('final_score', 0)
            merged_candidates[key]['matched_queries'].add(res['matched_query'])
            if merged_candidates[key]['data'] is None: merged_candidates[key]['data'] = res
        
        final_results = []
        for key, value in merged_candidates.items():
            final_data = value['data']
            num_matches = len(value['matched_queries'])
            sum_score = value['sum_score']
            final_score = (num_matches ** 1.5) * sum_score # C√¥ng th·ª©c ƒëi·ªÉm th∆∞·ªüng cho s·ª± ƒë·ªìng xu·∫•t hi·ªán
            final_data['final_score'] = final_score
            final_data['matched_queries'] = list(value['matched_queries'])
            final_results.append(final_data)
        
        final_results.sort(key=lambda x: x['final_score'], reverse=True)
        print(f"   -> H·ª£p nh·∫•t c√≤n {len(final_results)} k·∫øt qu·∫£ ƒë·ªôc nh·∫•t.")

        # === B∆Ø·ªöC 4: HO√ÄN THI·ªÜN V√Ä TR·∫¢ V·ªÄ ===
        top_n_before_dedup = final_results[:kis_retrieval_count]
        deduplicated_results = self._deduplicate_temporally(top_n_before_dedup)
        
        lambda_mmr = config.get('lambda_mmr_slider', 0.7)
        diverse_results = deduplicated_results
        if self.mmr_builder and diverse_results:
             print(f"   -> √Åp d·ª•ng MMR (Œª={lambda_mmr}) ƒë·ªÉ ƒëa d·∫°ng h√≥a k·∫øt qu·∫£...")
             diverse_results = self.mmr_builder.build_diverse_list(
                 candidates=diverse_results, target_size=len(diverse_results), lambda_val=lambda_mmr
             )
        
        top_k_final = int(config.get('num_results', 100))
        final_results_for_submission = diverse_results[:top_k_final]
        
        if self.video_path_map:
            for result in final_results_for_submission:
                result['video_path'] = self.video_path_map.get(result.get('video_id'))

        print(f"--- ‚úÖ Chi·∫øn d·ªãch PHOENIX REBORN ho√†n t·∫•t. Tr·∫£ v·ªÅ {len(final_results_for_submission)} k·∫øt qu·∫£. ---")
        
        return {
            "task_type": TaskType.KIS, # Lu√¥n l√† KIS
            "results": final_results_for_submission,
            "query_analysis": {"sub_queries": sub_queries}
        }