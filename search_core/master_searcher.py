# search_core/master_searcher.py
from typing import Dict, Any, Optional, List
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from tqdm import tqdm
from search_core.basic_searcher import BasicSearcher
from search_core.semantic_searcher import SemanticSearcher
from search_core.trake_solver import TRAKESolver
from search_core.gemini_text_handler import GeminiTextHandler
from search_core.openai_handler import OpenAIHandler
from search_core.task_analyzer import TaskType
from search_core.mmr_builder import MMRResultBuilder 


class MasterSearcher:
    """
    L·ªõp ƒëi·ªÅu ph·ªëi ch√≠nh c·ªßa h·ªá th·ªëng t√¨m ki·∫øm (Hybrid AI Edition).
    N√≥ qu·∫£n l√Ω v√† ƒëi·ªÅu ph·ªëi c√°c AI Handler kh√°c nhau (Gemini cho text, OpenAI cho vision)
    ƒë·ªÉ gi·∫£i quy·∫øt c√°c lo·∫°i truy v·∫•n ph·ª©c t·∫°p.
    """

    def __init__(self, 
                 basic_searcher: BasicSearcher, 
                 rerank_model,
                 gemini_api_key: Optional[str] = None,
                 openai_api_key: Optional[str] = None,
                 entities_path: str = None,
                 clip_features_path: str = None,
                 video_path_map: dict = None):
        """
        Kh·ªüi t·∫°o MasterSearcher v√† h·ªá sinh th√°i AI lai.
        """
        print("--- üß† Kh·ªüi t·∫°o Master Searcher (Hybrid AI Edition) ---")
        
        self.semantic_searcher = SemanticSearcher(basic_searcher=basic_searcher, rerank_model=rerank_model)
        self.mmr_builder: Optional[MMRResultBuilder] = None
        if clip_features_path and os.path.exists(clip_features_path):
            try:
                print(f"--- üöö ƒêang t·∫£i to√†n b·ªô CLIP features cho MMR t·ª´: {clip_features_path} ---")
                all_clip_features = np.load(clip_features_path)
                self.mmr_builder = MMRResultBuilder(clip_features=all_clip_features)
            except Exception as e:
                 print(f"--- ‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o MMR Builder: {e}. MMR s·∫Ω b·ªã v√¥ hi·ªáu h√≥a. ---")
        else:
            print("--- ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file CLIP features, MMR s·∫Ω kh√¥ng ho·∫°t ƒë·ªông. ---")
        self.video_path_map = video_path_map
        self.gemini_handler: Optional[GeminiTextHandler] = None
        self.openai_handler: Optional[OpenAIHandler] = None
        self.trake_solver: Optional[TRAKESolver] = None
        self.ai_enabled = False
        self.known_entities: set = set()
        print(f"--- ‚úÖ Master Searcher ƒë√£ s·∫µn s√†ng! (AI Enabled: {self.ai_enabled}) ---")
        
        if entities_path and os.path.exists(entities_path):
            try:
                print(f"--- üìö ƒêang t·∫£i T·ª´ ƒëi·ªÉn ƒê·ªëi t∆∞·ª£ng t·ª´: {entities_path} ---")
                with open(entities_path, 'r') as f:
                    entities_list = [entity.lower() for entity in json.load(f)]
                    self.known_entities = set(entities_list)
                print(f"--- ‚úÖ T·∫£i th√†nh c√¥ng {len(self.known_entities)} th·ª±c th·ªÉ ƒë√£ bi·∫øt. ---")
            except Exception as e:
                print(f"--- ‚ö†Ô∏è L·ªói khi t·∫£i T·ª´ ƒëi·ªÉn ƒê·ªëi t∆∞·ª£ng: {e}. Semantic Grounding s·∫Ω b·ªã v√¥ hi·ªáu h√≥a. ---")
        if gemini_api_key:
            try:
                self.gemini_handler = GeminiTextHandler(api_key=gemini_api_key)
                if self.known_entities and self.gemini_handler:
                    self.gemini_handler.load_known_entities(self.known_entities)
                self.ai_enabled = True
            except Exception as e:
                print(f"--- ‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o Gemini Handler: {e}. C√°c t√≠nh nƒÉng text AI s·∫Ω b·ªã h·∫°n ch·∫ø. ---")
        if openai_api_key:
            try:
                self.openai_handler = OpenAIHandler(api_key=openai_api_key)
                if not self.openai_handler.check_api_health():
                    self.openai_handler = None
                else:
                    self.ai_enabled = True
            except Exception as e:
                print(f"--- ‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o OpenAI Handler: {e}. C√°c t√≠nh nƒÉng vision AI s·∫Ω b·ªã h·∫°n ch·∫ø. ---")
        if self.gemini_handler:
            self.trake_solver = TRAKESolver(ai_handler=self.gemini_handler)

        print(f"--- ‚úÖ Master Searcher ƒë√£ s·∫µn s√†ng! (AI Enabled: {self.ai_enabled}) ---")
        
    def perform_semantic_grounding(self, entities_to_ground: List[str]) -> Dict[str, str]:
        """
        D·ªãch c√°c nh√£n entity t·ª± do v·ªÅ c√°c nh√£n chu·∫©n c√≥ trong t·ª´ ƒëi·ªÉn.
        Tr·∫£ v·ªÅ m·ªôt dictionary mapping: {entity_g·ªëc: entity_ƒë√£_d·ªãch}.
        """
        if not entities_to_ground or not self.known_entities_prompt_segment:
            return {}

        print(f"--- üß† B·∫Øt ƒë·∫ßu Semantic Grounding cho: {entities_to_ground} ---")
        
        prompt = (
            f"You are a helpful assistant. Your task is to map a list of input entities to the closest matching entities from a predefined dictionary. "
            f"For each input entity, find the single most appropriate term from the dictionary.\n\n"
            f"**Predefined Dictionary:**\n{self.known_entities_prompt_segment}\n\n"
            f"**Input Entities to Map:**\n{json.dumps(entities_to_ground)}\n\n"
            f"Provide your answer ONLY as a valid JSON object mapping each input entity to its corresponding dictionary term. "
            f"Example format: {{\"input_entity_1\": \"dictionary_term_1\", \"input_entity_2\": \"dictionary_term_2\"}}"
        )
        
        try:
            response = self.model.generate_content(prompt)
            grounding_map = json.loads(response.text)
            print(f"    -> K·∫øt qu·∫£ Grounding: {grounding_map}")
            return grounding_map
        except Exception as e:
            print(f"--- ‚ö†Ô∏è L·ªói trong qu√° tr√¨nh Semantic Grounding: {e} ---")
            return {}
    
    def _deduplicate_temporally(self, results: List[Dict[str, Any]], time_threshold: int = 5) -> List[Dict[str, Any]]:
        """
        L·ªçc c√°c k·∫øt qu·∫£ b·ªã tr√πng l·∫∑p v·ªÅ m·∫∑t th·ªùi gian trong c√πng m·ªôt video.

        Args:
            results (List[Dict[str, Any]]): Danh s√°ch k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒëi·ªÉm.
            time_threshold (int): Ng∆∞·ª°ng th·ªùi gian (gi√¢y). C√°c frame trong c√πng video
                                  c√°ch nhau d∆∞·ªõi ng∆∞·ª°ng n√†y s·∫Ω b·ªã coi l√† tr√πng l·∫∑p.

        Returns:
            List[Dict[str, Any]]: Danh s√°ch k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l·ªçc.
        """
        if not results:
            return []
        print(f"--- üõ°Ô∏è B·∫Øt ƒë·∫ßu L·ªçc Tr√πng l·∫∑p Th·ªùi gian (Ng∆∞·ª°ng: {time_threshold}s)... ---")
        last_timestamp_per_video = {}
        deduplicated_results = []
        for result in results:
            video_id = result.get('video_id')
            timestamp = result.get('timestamp')
            if not video_id or timestamp is None:
                continue 
            last_seen_timestamp = last_timestamp_per_video.get(video_id)
            if last_seen_timestamp is None or abs(timestamp - last_seen_timestamp) > time_threshold:
                deduplicated_results.append(result)
                last_timestamp_per_video[video_id] = timestamp
        
        print(f"--- ‚úÖ L·ªçc ho√†n t·∫•t. T·ª´ {len(results)} -> c√≤n {len(deduplicated_results)} k·∫øt qu·∫£. ---")
        return deduplicated_results

    def search(self, query: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        H√†m t√¨m ki·∫øm ch√≠nh, nh·∫≠n m·ªôt dictionary config ƒë·ªÉ t√πy ch·ªânh h√†nh vi.
        """
        top_k_final = int(config.get('top_k_final', 100))
        kis_retrieval = int(config.get('kis_retrieval', 200))
        vqa_candidates_to_rank = int(config.get('vqa_candidates', 20))
        vqa_retrieval = int(config.get('vqa_retrieval', 200))
        trake_candidates_per_step = int(config.get('trake_candidates_per_step', 20))
        trake_max_sequences = int(config.get('trake_max_sequences', 50))
        w_clip = config.get('w_clip', 0.4)
        w_obj = config.get('w_obj', 0.3)
        w_semantic = config.get('w_semantic', 0.3)
        lambda_mmr = config.get('lambda_mmr', 0.7)

        query_analysis = {}
        task_type = TaskType.KIS
        if self.ai_enabled and self.gemini_handler:
            print("--- ‚ú® B·∫Øt ƒë·∫ßu ph√¢n t√≠ch truy v·∫•n b·∫±ng Gemini Text Handler... ---")
            query_analysis = self.gemini_handler.analyze_query_fully(query)
            
            entities_to_ground = query_analysis.get('entities_to_ground', [])
            if entities_to_ground:
                grounding_map = self.gemini_handler.perform_semantic_grounding(entities_to_ground)
                query_analysis['grounding_map'] = grounding_map
            else:
                query_analysis['grounding_map'] = {}
            
            original_objects = query_analysis.get('objects_en', [])
            if original_objects:
                grounded_objects = self.gemini_handler.perform_semantic_grounding(original_objects)
                if original_objects != grounded_objects:
                     print(f"--- üß† Semantic Grounding: {original_objects} -> {grounded_objects} ---")
                query_analysis['objects_en'] = grounded_objects
                
            task_type_str = query_analysis.get('task_type', 'KIS').upper()
            try:
                task_type = TaskType[task_type_str]
            except KeyError:
                task_type = TaskType.KIS
        
        print(f"--- ƒê√£ ph√¢n lo·∫°i truy v·∫•n l√†: {task_type.value} ---")

        final_results = []
        query_analysis.update({'w_clip': w_clip, 'w_obj': w_obj, 'w_semantic': w_semantic})
        search_context = query_analysis.get('search_context', query)

        if task_type == TaskType.TRAKE:
            if self.trake_solver:
                sub_queries = self.trake_solver.decompose_query(query)
                final_results = self.trake_solver.find_sequences(
                    sub_queries, 
                    self.semantic_searcher,
                    original_query_analysis=query_analysis,
                    top_k_per_step=trake_candidates_per_step,
                    max_sequences=trake_max_sequences
                )
            else:
                task_type = TaskType.KIS

        elif task_type == TaskType.QNA:
            if self.openai_handler:
                candidates = self.semantic_searcher.search(
                    query_text=search_context,
                    precomputed_analysis=query_analysis,
                    top_k_final=vqa_retrieval,
                    top_k_retrieval=vqa_retrieval
                )
                
                if not candidates:
                    final_results = []
                else:
                    candidates_for_vqa = candidates[:vqa_candidates_to_rank]
                    specific_question = query_analysis.get('specific_question', query)
                    vqa_enhanced_candidates = []
                    
                    print(f"--- üí¨ B·∫Øt ƒë·∫ßu Qu√©t VQA song song tr√™n {len(candidates_for_vqa)} ·ª©ng vi√™n... ---")
                    
                    with ThreadPoolExecutor(max_workers=8) as executor:
                        future_to_candidate = {
                            executor.submit(
                                self.openai_handler.perform_vqa, 
                                image_path=cand['keyframe_path'], 
                                question=specific_question, 
                                context_text=cand.get('transcript_text', '')
                            ): cand 
                            for cand in candidates_for_vqa
                        }
                        
                        for future in tqdm(as_completed(future_to_candidate), total=len(candidates_for_vqa), desc="   -> VQA Progress"):
                            cand = future_to_candidate[future]
                            try:
                                vqa_result = future.result()
                                new_cand = cand.copy()
                                new_cand['answer'] = vqa_result['answer']
                                search_score = new_cand.get('final_score', 0)
                                vqa_confidence = vqa_result.get('confidence', 0)
                                new_cand['final_score'] = search_score * vqa_confidence
                                new_cand['scores']['vqa_confidence'] = vqa_confidence
                                vqa_enhanced_candidates.append(new_cand)
                            except Exception as exc:
                                print(f"--- ‚ùå L·ªói khi x·ª≠ l√Ω VQA cho keyframe {cand.get('keyframe_id')}: {exc} ---")
                    
                    if vqa_enhanced_candidates:
                        final_results = sorted(vqa_enhanced_candidates, key=lambda x: x['final_score'], reverse=True)
                    else:
                        final_results = []
            else:
                print("--- ‚ö†Ô∏è OpenAI (VQA) handler ch∆∞a ƒë∆∞·ª£c k√≠ch ho·∫°t. Fallback v·ªÅ KIS. ---")
                task_type = TaskType.KIS

        if not final_results or task_type == TaskType.KIS:
            final_results = self.semantic_searcher.search(
                query_text=search_context,
                precomputed_analysis=query_analysis,
                top_k_final=kis_retrieval, 
                top_k_retrieval=kis_retrieval
            )
        if task_type in [TaskType.KIS, TaskType.QNA]:
            final_results = self._deduplicate_temporally(final_results, time_threshold=2)
        if self.video_path_map and task_type in [TaskType.KIS, TaskType.QNA]:
            for result in final_results:
                result['video_path'] = self.video_path_map.get(result.get('video_id'))
        diverse_results = final_results
        # if self.mmr_builder and final_results:
        #     if task_type in [TaskType.KIS, TaskType.QNA]:
        #         diverse_results = self.mmr_builder.build_diverse_list(
        #             candidates=final_results, 
        #             target_size=len(final_results),
        #             lambda_val=lambda_mmr
        #         )
        final_results_for_submission = diverse_results[:top_k_final]
        print("\n" + "="*20 + " DEBUG LOG: MASTER SEARCHER OUTPUT " + "="*20)
        print(f"-> Task Type cu·ªëi c√πng: {task_type.value}")
        print(f"-> S·ªë l∆∞·ª£ng k·∫øt qu·∫£ cu·ªëi c√πng: {len(final_results)}")
        if final_results:
            print("-> V√≠ d·ª• k·∫øt qu·∫£ ƒë·∫ßu ti√™n:")
            first_result = final_results[0]
            if task_type == TaskType.TRAKE:
                print(f"  - video_id: {first_result.get('video_id')}")
                print(f"  - final_score: {first_result.get('final_score')}")
                print(f"  - S·ªë b∆∞·ªõc trong chu·ªói: {len(first_result.get('sequence', []))}")
            else:
                print(f"  - keyframe_id: {first_result.get('keyframe_id')}")
                print(f"  - final_score: {first_result.get('final_score')}")
                if 'answer' in first_result:
                    print(f"  - answer: {first_result.get('answer')}")
        else:
            print("-> Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra.")
        print("="*68 + "\n")
        
        return {
            "task_type": task_type,
            "results": final_results_for_submission,
            "query_analysis": query_analysis
        }