# ==============================================================================
# SEMANTIC SEARCHER - PHI√äN B·∫¢N V5 (T·ªêI ∆ØU H√ìA & N√ÇNG C·∫§P T·ª™ B·∫¢N G·ªêC)
# ==============================================================================
import os
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
import re
import torch
from tqdm import tqdm
from typing import Dict, List, Optional, Any
from utils.cache_manager import ObjectVectorCache
from utils.spatial_engine import is_between, is_behind
from utils.image_cropper import crop_image_by_box
from search_core.basic_searcher import BasicSearcher

class SemanticSearcher:
    def __init__(self, basic_searcher, rerank_model, device="cuda"):
        print("--- üß† Kh·ªüi t·∫°o SemanticSearcher (Reranking Engine - Phoenix Edition) ---")
        self.basic_searcher = basic_searcher
        self.model = rerank_model
        self.device = device
        
        # --- T·∫¢I "H·ªí D·ªÆ LI·ªÜU OBJECT" ---
        self.master_object_df = None
        object_data_path = "/kaggle/input/stage1/master_object_data.parquet"
        if os.path.exists(object_data_path):
            print(f"   -> ƒêang t·∫£i H·ªì D·ªØ li·ªáu Object t·ª´: {object_data_path}")
            self.master_object_df = pd.read_parquet(object_data_path)
            # T·ªëi ∆∞u h√≥a: Set index ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô truy v·∫•n sau n√†y
            self.master_object_df.set_index('keyframe_id', inplace=True)
            print(f"--- ‚úÖ T·∫£i th√†nh c√¥ng v√† l·∫≠p ch·ªâ m·ª•c cho {len(self.master_object_df)} object. ---")
        else:
            print("--- ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y master_object_data.parquet. B·ªô l·ªçc kh√¥ng gian s·∫Ω b·ªã v√¥ hi·ªáu h√≥a. ---")
            
        # --- TRANG B·ªä C√îNG C·ª§ CHO T·∫¶NG 3 ---
        print("--- üî¨ Trang b·ªã c√¥ng c·ª• X√°c th·ª±c Chi ti·∫øt... ---")
        # L·∫•y quy·ªÅn truy c·∫≠p v√†o CLIP model v√† processor t·ª´ BasicSearcher
        self.clip_model = basic_searcher.model
        self.clip_processor = basic_searcher.processor
        # Kh·ªüi t·∫°o Ng√¢n h√†ng Vector Linh ho·∫°t
        self.object_vector_cache = ObjectVectorCache()
        print("--- ‚úÖ S·∫µn s√†ng ho·∫°t ƒë·ªông v·ªõi b·ªô nh·ªõ cache. ---")
            
    def _apply_spatial_filter(self, candidates: List[Dict], spatial_rules: List[Dict], grounded_entities: List[str]) -> List[Dict]:
        """
        √Åp d·ª•ng c√°c quy t·∫Øc kh√¥ng gian ƒë·ªÉ t√≠nh ƒëi·ªÉm 'spatial_score' cho m·ªói ·ª©ng vi√™n.
        PHI√äN B·∫¢N HO√ÄN CH·ªàNH.
        """
        # --- ƒêi·ªÅu ki·ªán tho√°t s·ªõm ---
        if not spatial_rules or self.master_object_df is None or self.master_object_df.empty:
            for cand in candidates:
                cand['scores']['spatial_score'] = 1.0
            return candidates

        print(f"--- üìê √Åp d·ª•ng {len(spatial_rules)} Quy t·∫Øc Kh√¥ng gian tr√™n {len(candidates)} ·ª©ng vi√™n... ---")
        
        candidate_ids = [c['keyframe_id'] for c in candidates]
        try:
            relevant_objects_df = self.master_object_df.loc[self.master_object_df.index.isin(candidate_ids)]
        except KeyError:
            relevant_objects_df = pd.DataFrame()

        if relevant_objects_df.empty:
            for cand in candidates:
                cand['scores']['spatial_score'] = 0.0
            return candidates

        for cand in candidates:
            keyframe_objects = relevant_objects_df[relevant_objects_df.index == cand['keyframe_id']]
            
            if keyframe_objects.empty:
                cand['scores']['spatial_score'] = 0.0
                continue
            
            total_rules = len(spatial_rules)
            satisfied_rules_count = 0
            
            # L·∫∑p qua t·ª´ng quy t·∫Øc m√† Gemini ƒë√£ cung c·∫•p
            for rule in spatial_rules:
                entity_label = rule['entity'].replace('_', ' ')
                relation = rule['relation']
                target_labels = [t.replace('_', ' ') for t in rule['targets']]
                
                # L·∫•y ra t·∫•t c·∫£ c√°c bounding box c·ªßa c√°c object c√≥ li√™n quan trong rule n√†y
                # Ch√∫ng ta s·∫Ω t√¨m c√°c label ch·ª©a (contains) entity_label, v√≠ d·ª• "man" s·∫Ω kh·ªõp v·ªõi "man black shirt"
                entity_boxes = keyframe_objects[keyframe_objects['object_label'].str.contains(entity_label, case=False)]['bounding_box'].tolist()
                
                target_boxes_lists = []
                for label in target_labels:
                    boxes = keyframe_objects[keyframe_objects['object_label'].str.contains(label, case=False)]['bounding_box'].tolist()
                    target_boxes_lists.append(boxes)

                # N·∫øu thi·∫øu b·∫•t k·ª≥ lo·∫°i object n√†o, kh√¥ng th·ªÉ th·ªèa m√£n rule -> b·ªè qua
                if not entity_boxes or any(not boxes for boxes in target_boxes_lists):
                    continue
                    
                rule_satisfied = False
                # L·∫∑p qua t·∫•t c·∫£ c√°c box c·ªßa entity ch√≠nh
                for entity_box in entity_boxes:
                    if rule_satisfied: break
                    
                    # --- X·ª≠ l√Ω c√°c lo·∫°i quan h·ªá ---
                    if relation == 'is_between' and len(target_boxes_lists) == 2:
                        # C·∫ßn t√¨m m·ªôt c·∫∑p target (t·ª´ list 1 v√† list 2) ƒë·ªÉ entity n·∫±m gi·ªØa
                        # L·∫•y t·∫•t c·∫£ c√°c c·∫∑p c√≥ th·ªÉ c√≥ gi·ªØa hai list target boxes
                        target_pairs = [(b1, b2) for b1 in target_boxes_lists[0] for b2 in target_boxes_lists[1]]
                        for target1_box, target2_box in target_pairs:
                            # Tr√°nh tr∆∞·ªùng h·ª£p 2 target l√† c√πng m·ªôt object
                            if target1_box == target2_box: continue
                            if is_between(entity_box, target1_box, target2_box):
                                rule_satisfied = True
                                break
                    
                    elif relation == 'is_behind' and len(target_boxes_lists) == 1:
                        for target_box in target_boxes_lists[0]:
                            if is_behind(entity_box, target_box):
                                rule_satisfied = True
                                break
                    
                    # TODO: Th√™m c√°c ƒëi·ªÅu ki·ªán 'is_next_to', 'is_above', etc. ·ªü ƒë√¢y n·∫øu c·∫ßn
                    
                if rule_satisfied:
                    satisfied_rules_count += 1
            
            # T√≠nh ƒëi·ªÉm cu·ªëi c√πng: t·ª∑ l·ªá c√°c rule ƒë∆∞·ª£c th·ªèa m√£n
            cand['scores']['spatial_score'] = satisfied_rules_count / total_rules if total_rules > 0 else 1.0

        # In ra m·ªôt v√†i v√≠ d·ª• ƒëi·ªÉm ƒë·ªÉ debug
        print("    -> V√≠ d·ª• ƒëi·ªÉm kh√¥ng gian:", {c['keyframe_id']: f"{c['scores']['spatial_score']:.2f}" for c in candidates[:5]})
        return candidates
    
    def _apply_fine_grained_filter(self, candidates: List[Dict], verification_rules: List[Dict]) -> List[Dict]:
        """
        S·ª≠ d·ª•ng CLIP tr√™n c√°c v√πng ·∫£nh ƒë√£ crop ƒë·ªÉ x√°c th·ª±c c√°c chi ti·∫øt nh·ªè.
        """
        if not verification_rules or self.master_object_df is None or self.master_object_df.empty:
            for cand in candidates:
                cand['scores']['fine_grained_score'] = 1.0
            return candidates

        print(f"--- üî¨ √Åp d·ª•ng {len(verification_rules)} Quy t·∫Øc X√°c th·ª±c Chi ti·∫øt...")
        
        # Ch·ªâ x·ª≠ l√Ω tr√™n top 50 ·ª©ng vi√™n ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian, s·ªë c√≤n l·∫°i nh·∫≠n ƒëi·ªÉm m·∫∑c ƒë·ªãnh
        top_candidates = candidates[:50]
        
        # Encode t·∫•t c·∫£ c√°c m√¥ t·∫£ text m·ªôt l·∫ßn duy nh·∫•t
        detailed_descriptions = [rule['detailed_description'] for rule in verification_rules]
        text_inputs = self.clip_processor(text=detailed_descriptions, return_tensors="pt", padding=True, truncation=True).to(self.device)
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**text_inputs)
            text_features /= text_features.norm(dim=-1, keepdim=True)

        for cand in tqdm(top_candidates, desc="X√°c th·ª±c chi ti·∫øt (soi k√≠nh hi·ªÉn vi)"):
            keyframe_id = cand['keyframe_id']
            keyframe_objects = self.master_object_df.loc[self.master_object_df.index == keyframe_id]
            
            if keyframe_objects.empty:
                cand['scores']['fine_grained_score'] = 0.0
                continue
            
            total_score = 0.0
            for i, rule in enumerate(verification_rules):
                target_label = rule['target_entity']
                
                # T√¨m object ph√π h·ª£p nh·∫•t trong keyframe (confidence cao nh·∫•t)
                possible_objects = keyframe_objects[keyframe_objects['object_label'].str.contains(target_label, case=False)]
                if possible_objects.empty:
                    continue # B·ªè qua rule n√†y n·∫øu kh√¥ng c√≥ object kh·ªõp

                best_object = possible_objects.loc[possible_objects['confidence_score'].idxmax()]
                
                # --- LOGIC CACHING ---
                cache_key = f"{keyframe_id}_{target_label}_{best_object['confidence_score']:.4f}"
                object_vector = self.object_vector_cache.get(cache_key)
                
                if object_vector is None: # Cache miss
                    try:
                        cropped_image = crop_image_by_box(cand['keyframe_path'], best_object['bounding_box'])
                        image_input = self.clip_processor(images=cropped_image, return_tensors="pt").to(self.device)
                        with torch.no_grad():
                            image_features = self.clip_model.get_image_features(**image_input)
                            image_features /= image_features.norm(dim=-1, keepdim=True)
                        
                        object_vector = image_features.cpu().numpy()
                        self.object_vector_cache.set(cache_key, object_vector)
                    except Exception as e:
                        print(f"L·ªói khi x·ª≠ l√Ω ·∫£nh crop cho {keyframe_id}: {e}")
                        continue
                
                # T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng
                image_tensor = torch.from_numpy(object_vector).to(self.device)
                similarity = util.pytorch_cos_sim(image_tensor, text_features[i].unsqueeze(0))
                total_score += similarity.item()

            cand['scores']['fine_grained_score'] = total_score / len(verification_rules) if verification_rules else 1.0

        # G√°n ƒëi·ªÉm m·∫∑c ƒë·ªãnh cho c√°c ·ª©ng vi√™n kh√¥ng ƒë∆∞·ª£c check
        for cand in candidates[50:]:
            cand['scores']['fine_grained_score'] = 0.5 # ƒêi·ªÉm trung b√¨nh

        return candidates

    def search(self,
               query_text: str,
               top_k_final: int,
               top_k_retrieval: int,
               precomputed_analysis: Dict[str, Any] = None,
               weights: Dict[str, float] = None
              ) -> List[Dict[str, Any]]:
        """
        Th·ª±c hi·ªán t√¨m ki·∫øm v√† t√°i x·∫øp h·∫°ng ƒëa t·∫ßng theo ki·∫øn tr√∫c PHOENIX.
        Lu·ªìng x·ª≠ l√Ω: Contextual -> Spatial -> Fine-grained Verification.
        """
        print("\n--- üî± B·∫Øt ƒë·∫ßu quy tr√¨nh t√¨m ki·∫øm ƒëa t·∫ßng PHOENIX... ---")

        # --- B∆∞·ªõc 0: Chu·∫©n b·ªã ---
        if precomputed_analysis is None: precomputed_analysis = {}
        # ƒê·∫∑t tr·ªçng s·ªë m·∫∑c ƒë·ªãnh v√† cho ph√©p ghi ƒë√® t·ª´ UI
        final_weights = {
            'w_clip': 0.2, 
            'w_semantic': 0.3, 
            'w_spatial': 0.25, 
            'w_fine_grained': 0.25, 
            **(weights or {})
        }
        print(f"    -> Tr·ªçng s·ªë h·ªèa l·ª±c: {final_weights}")

        # --- T·∫¶NG 1: B·ªò L·ªåC NG·ªÆ C·∫¢NH (L·∫•y ·ª©ng vi√™n th√¥ b·∫±ng CLIP to√†n c·ª•c) ---
        print(f"--- T·∫ßng 1: L·∫•y Top-{top_k_retrieval} ·ª©ng vi√™n theo Ng·ªØ c·∫£nh... ---")
        candidates = self.basic_searcher.search(query_text, top_k=top_k_retrieval)
        if not candidates:
            print("--- ‚õî Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n n√†o ·ªü T·∫ßng 1. D·ª´ng t√¨m ki·∫øm. ---")
            return []
        print(f"    -> T√¨m th·∫•y {len(candidates)} ·ª©ng vi√™n ti·ªÅm nƒÉng.")
        
        # Kh·ªüi t·∫°o c·∫•u tr√∫c ƒëi·ªÉm cho m·ªói ·ª©ng vi√™n
        for cand in candidates:
            cand['scores'] = {'clip_score': cand.get('clip_score', 0.0)}

        # --- RERANKING NG·ªÆ NGHƒ®A (Tinh ch·ªânh ƒëi·ªÉm ng·ªØ c·∫£nh b·∫±ng Bi-Encoder) ---
        # (Ph·∫ßn n√†y b·∫°n c·∫ßn ƒë·∫£m b·∫£o logic rerank_batch c·ªßa m√¨nh ƒë∆∞·ª£c t√≠ch h·ª£p ·ªü ƒë√¢y)
        # Gi·∫£ s·ª≠ sau b∆∞·ªõc n√†y, 'semantic_score' ƒë∆∞·ª£c th√™m v√†o
        print("--- T·∫ßng 1.5: Tinh ch·ªânh ƒëi·ªÉm Ng·ªØ nghƒ©a b·∫±ng Bi-Encoder... ---")
        # V√≠ d·ª•:
        # candidates = self.rerank_with_bi_encoder(candidates, query_text)
        # T·∫°m th·ªùi g√°n ƒëi·ªÉm gi·∫£ ƒë·ªãnh ƒë·ªÉ code ch·∫°y ƒë∆∞·ª£c
        for cand in candidates:
             cand['scores']['semantic_score'] = cand['scores']['clip_score'] # T·∫°m th·ªùi g√°n b·∫±ng ƒëi·ªÉm clip
        print("    -> Ho√†n t·∫•t tinh ch·ªânh ƒëi·ªÉm ng·ªØ nghƒ©a.")


        # --- T·∫¶NG 2: B·ªò L·ªåC QUAN H·ªÜ KH√îNG GIAN ---
        spatial_rules = precomputed_analysis.get('spatial_rules', [])
        grounded_entities = precomputed_analysis.get('grounded_entities', []) # C√≥ th·ªÉ d√πng trong t∆∞∆°ng lai
        
        candidates_after_spatial = self._apply_spatial_filter(candidates, spatial_rules, grounded_entities)


        # --- T·∫¶NG 3: B·ªò L·ªåC X√ÅC TH·ª∞C CHI TI·∫æT ---
        verification_rules = precomputed_analysis.get('fine_grained_verification', [])
        
        # S·∫Øp x·∫øp l·∫°i tr∆∞·ªõc khi ƒë∆∞a v√†o T·∫ßng 3 ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ "soi" nh·ªØng ·ª©ng vi√™n t·ªët nh·∫•t
        # T√≠nh ƒëi·ªÉm t·∫°m th·ªùi sau T·∫ßng 2
        for cand in candidates_after_spatial:
            s = cand['scores']
            cand['temp_score'] = (
                final_weights['w_clip'] * s.get('clip_score', 0.0) +
                final_weights['w_semantic'] * s.get('semantic_score', 0.0) +
                final_weights['w_spatial'] * s.get('spatial_score', 0.5)
            )
        
        sorted_before_fine_grained = sorted(candidates_after_spatial, key=lambda x: x.get('temp_score', 0.0), reverse=True)

        candidates_after_fine_grained = self._apply_fine_grained_filter(sorted_before_fine_grained, verification_rules)


        # --- B∆Ø·ªöC CU·ªêI: T√çNH ƒêI·ªÇM T·ªîNG H·ª¢P V√Ä S·∫ÆP X·∫æP ---
        print("--- üéØ T√≠nh to√°n ƒëi·ªÉm h·ªèa l·ª±c cu·ªëi c√πng v√† s·∫Øp x·∫øp... ---")
        for cand in candidates_after_fine_grained:
            scores = cand['scores']
            
            # C√¥ng th·ª©c ƒëi·ªÉm ho√†n ch·ªânh
            final_score = (
                final_weights['w_clip'] * scores.get('clip_score', 0.0) +
                final_weights['w_semantic'] * scores.get('semantic_score', 0.0) +
                final_weights['w_spatial'] * scores.get('spatial_score', 0.5) + # D√πng 0.5 l√†m ƒëi·ªÉm m·∫∑c ƒë·ªãnh n·∫øu c√≥ l·ªói
                final_weights['w_fine_grained'] * scores.get('fine_grained_score', 0.5) # D√πng 0.5 l√†m ƒëi·ªÉm m·∫∑c ƒë·ªãnh
            )
            cand['final_score'] = final_score

        # S·∫Øp x·∫øp l·∫°i l·∫ßn cu·ªëi c√πng d·ª±a tr√™n ƒëi·ªÉm t·ªïng h·ª£p
        final_sorted_candidates = sorted(candidates_after_fine_grained, key=lambda x: x.get('final_score', 0.0), reverse=True)
        
        print(f"--- ‚úÖ Quy tr√¨nh PHOENIX ho√†n t·∫•t. Tr·∫£ v·ªÅ Top-{top_k_final} k·∫øt qu·∫£. ---")
        
        # In ra 3 k·∫øt qu·∫£ ƒë·∫ßu ti√™n ƒë·ªÉ debug
        for i, cand in enumerate(final_sorted_candidates[:3]):
            print(f"  Top {i+1}: {cand['keyframe_id']} | Score: {cand['final_score']:.4f} | Scores: {cand['scores']}")

        return final_sorted_candidates[:top_k_final]