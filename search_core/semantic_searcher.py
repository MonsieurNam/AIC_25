# ==============================================================================
# SEMANTIC SEARCHER - PHIÃŠN Báº¢N V5 (Tá»I Æ¯U HÃ“A & NÃ‚NG Cáº¤P Tá»ª Báº¢N Gá»C)
# ==============================================================================
import os
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
import re
import torch
from tqdm import tqdm
from typing import Dict, List, Optional, Any
from utils.cache_manager import ObjectVectorCache
from utils.spatial_engine import is_between, is_behind
from utils.image_cropper import crop_image_by_box
from search_core.basic_searcher import BasicSearcher

class SemanticSearcher:
    def __init__(self, basic_searcher, rerank_model, device="cuda"):
        print("--- ğŸ§  Khá»Ÿi táº¡o SemanticSearcher (Reranking Engine - Phoenix Edition) ---")
        self.basic_searcher = basic_searcher
        self.model = rerank_model
        self.device = device
        
        # --- Táº¢I "Há»’ Dá»® LIá»†U OBJECT" ---
        self.master_object_df = None
        object_data_path = "/kaggle/input/stage1/master_object_data.parquet"
        if os.path.exists(object_data_path):
            print(f"   -> Äang táº£i Há»“ Dá»¯ liá»‡u Object tá»«: {object_data_path}")
            self.master_object_df = pd.read_parquet(object_data_path)
            # Tá»‘i Æ°u hÃ³a: Set index Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ truy váº¥n sau nÃ y
            self.master_object_df.set_index('keyframe_id', inplace=True)
            print(f"--- âœ… Táº£i thÃ nh cÃ´ng vÃ  láº­p chá»‰ má»¥c cho {len(self.master_object_df)} object. ---")
        else:
            print("--- âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y master_object_data.parquet. Bá»™ lá»c khÃ´ng gian sáº½ bá»‹ vÃ´ hiá»‡u hÃ³a. ---")
            
        # --- TRANG Bá»Š CÃ”NG Cá»¤ CHO Táº¦NG 3 ---
        print("--- ğŸ”¬ Trang bá»‹ cÃ´ng cá»¥ XÃ¡c thá»±c Chi tiáº¿t... ---")
        # Láº¥y quyá»n truy cáº­p vÃ o CLIP model vÃ  processor tá»« BasicSearcher
        self.clip_model = basic_searcher.model
        self.clip_processor = basic_searcher.processor
        # Khá»Ÿi táº¡o NgÃ¢n hÃ ng Vector Linh hoáº¡t
        self.object_vector_cache = ObjectVectorCache()
        print("--- âœ… Sáºµn sÃ ng hoáº¡t Ä‘á»™ng vá»›i bá»™ nhá»› cache. ---")
            
    def _apply_spatial_filter(self, candidates: List[Dict], spatial_rules: List[Dict], grounded_entities: List[str]) -> List[Dict]:
        """
        Ãp dá»¥ng cÃ¡c quy táº¯c khÃ´ng gian Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm 'spatial_score' cho má»—i á»©ng viÃªn.
        PHIÃŠN Báº¢N HOÃ€N CHá»ˆNH.
        """
        # --- Äiá»u kiá»‡n thoÃ¡t sá»›m ---
        if not spatial_rules or self.master_object_df is None or self.master_object_df.empty:
            for cand in candidates:
                cand['scores']['spatial_score'] = 1.0
            return candidates

        print(f"--- ğŸ“ Ãp dá»¥ng {len(spatial_rules)} Quy táº¯c KhÃ´ng gian trÃªn {len(candidates)} á»©ng viÃªn... ---")
        
        candidate_ids = [c['keyframe_id'] for c in candidates]
        try:
            relevant_objects_df = self.master_object_df.loc[self.master_object_df.index.isin(candidate_ids)]
        except KeyError:
            relevant_objects_df = pd.DataFrame()

        if relevant_objects_df.empty:
            for cand in candidates:
                cand['scores']['spatial_score'] = 0.0
            return candidates

        for cand in candidates:
            keyframe_objects = relevant_objects_df[relevant_objects_df.index == cand['keyframe_id']]
            
            if keyframe_objects.empty:
                cand['scores']['spatial_score'] = 0.0
                continue
            
            total_rules = len(spatial_rules)
            satisfied_rules_count = 0
            
            # Láº·p qua tá»«ng quy táº¯c mÃ  Gemini Ä‘Ã£ cung cáº¥p
            for rule in spatial_rules:
                entity_label = rule['entity'].replace('_', ' ')
                relation = rule['relation']
                target_labels = [t.replace('_', ' ') for t in rule['targets']]
                
                # Láº¥y ra táº¥t cáº£ cÃ¡c bounding box cá»§a cÃ¡c object cÃ³ liÃªn quan trong rule nÃ y
                # ChÃºng ta sáº½ tÃ¬m cÃ¡c label chá»©a (contains) entity_label, vÃ­ dá»¥ "man" sáº½ khá»›p vá»›i "man black shirt"
                entity_boxes = keyframe_objects[keyframe_objects['object_label'].str.contains(entity_label, case=False)]['bounding_box'].tolist()
                
                target_boxes_lists = []
                for label in target_labels:
                    boxes = keyframe_objects[keyframe_objects['object_label'].str.contains(label, case=False)]['bounding_box'].tolist()
                    target_boxes_lists.append(boxes)

                # Náº¿u thiáº¿u báº¥t ká»³ loáº¡i object nÃ o, khÃ´ng thá»ƒ thá»a mÃ£n rule -> bá» qua
                if not entity_boxes or any(not boxes for boxes in target_boxes_lists):
                    continue
                    
                rule_satisfied = False
                # Láº·p qua táº¥t cáº£ cÃ¡c box cá»§a entity chÃ­nh
                for entity_box in entity_boxes:
                    if rule_satisfied: break
                    
                    # --- Xá»­ lÃ½ cÃ¡c loáº¡i quan há»‡ ---
                    if relation == 'is_between' and len(target_boxes_lists) == 2:
                        # Cáº§n tÃ¬m má»™t cáº·p target (tá»« list 1 vÃ  list 2) Ä‘á»ƒ entity náº±m giá»¯a
                        # Láº¥y táº¥t cáº£ cÃ¡c cáº·p cÃ³ thá»ƒ cÃ³ giá»¯a hai list target boxes
                        target_pairs = [(b1, b2) for b1 in target_boxes_lists[0] for b2 in target_boxes_lists[1]]
                        for target1_box, target2_box in target_pairs:
                            # TrÃ¡nh trÆ°á»ng há»£p 2 target lÃ  cÃ¹ng má»™t object
                            if target1_box == target2_box: continue
                            if is_between(entity_box, target1_box, target2_box):
                                rule_satisfied = True
                                break
                    
                    elif relation == 'is_behind' and len(target_boxes_lists) == 1:
                        for target_box in target_boxes_lists[0]:
                            if is_behind(entity_box, target_box):
                                rule_satisfied = True
                                break
                    
                    # TODO: ThÃªm cÃ¡c Ä‘iá»u kiá»‡n 'is_next_to', 'is_above', etc. á»Ÿ Ä‘Ã¢y náº¿u cáº§n
                    
                if rule_satisfied:
                    satisfied_rules_count += 1
            
            # TÃ­nh Ä‘iá»ƒm cuá»‘i cÃ¹ng: tá»· lá»‡ cÃ¡c rule Ä‘Æ°á»£c thá»a mÃ£n
            cand['scores']['spatial_score'] = satisfied_rules_count / total_rules if total_rules > 0 else 1.0

        # In ra má»™t vÃ i vÃ­ dá»¥ Ä‘iá»ƒm Ä‘á»ƒ debug
        print("    -> VÃ­ dá»¥ Ä‘iá»ƒm khÃ´ng gian:", {c['keyframe_id']: f"{c['scores']['spatial_score']:.2f}" for c in candidates[:5]})
        return candidates
    
    def _apply_fine_grained_filter(self, candidates: List[Dict], verification_rules: List[Dict]) -> List[Dict]:
        """
        Sá»­ dá»¥ng CLIP trÃªn cÃ¡c vÃ¹ng áº£nh Ä‘Ã£ crop Ä‘á»ƒ xÃ¡c thá»±c cÃ¡c chi tiáº¿t nhá».
        """
        if not verification_rules or self.master_object_df is None or self.master_object_df.empty:
            for cand in candidates:
                cand['scores']['fine_grained_score'] = 1.0
            return candidates

        print(f"--- ğŸ”¬ Ãp dá»¥ng {len(verification_rules)} Quy táº¯c XÃ¡c thá»±c Chi tiáº¿t...")
        
        # Chá»‰ xá»­ lÃ½ trÃªn top 50 á»©ng viÃªn Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian, sá»‘ cÃ²n láº¡i nháº­n Ä‘iá»ƒm máº·c Ä‘á»‹nh
        top_candidates = candidates[:50]
        
        # Encode táº¥t cáº£ cÃ¡c mÃ´ táº£ text má»™t láº§n duy nháº¥t
        detailed_descriptions = [rule['detailed_description'] for rule in verification_rules]
        text_inputs = self.clip_processor(text=detailed_descriptions, return_tensors="pt", padding=True, truncation=True).to(self.device)
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**text_inputs)
            text_features /= text_features.norm(dim=-1, keepdim=True)

        for cand in tqdm(top_candidates, desc="XÃ¡c thá»±c chi tiáº¿t (soi kÃ­nh hiá»ƒn vi)"):
            keyframe_id = cand['keyframe_id']
            keyframe_objects = self.master_object_df.loc[self.master_object_df.index == keyframe_id]
            
            if keyframe_objects.empty:
                cand['scores']['fine_grained_score'] = 0.0
                continue
            
            total_score = 0.0
            for i, rule in enumerate(verification_rules):
                target_label = rule['target_entity']
                
                # TÃ¬m object phÃ¹ há»£p nháº¥t trong keyframe (confidence cao nháº¥t)
                possible_objects = keyframe_objects[keyframe_objects['object_label'].str.contains(target_label, case=False)]
                if possible_objects.empty:
                    continue # Bá» qua rule nÃ y náº¿u khÃ´ng cÃ³ object khá»›p

                best_object = possible_objects.loc[possible_objects['confidence_score'].idxmax()]
                
                # --- LOGIC CACHING ---
                cache_key = f"{keyframe_id}_{target_label}_{best_object['confidence_score']:.4f}"
                object_vector = self.object_vector_cache.get(cache_key)
                
                if object_vector is None: # Cache miss
                    try:
                        cropped_image = crop_image_by_box(cand['keyframe_path'], best_object['bounding_box'])
                        image_input = self.clip_processor(images=cropped_image, return_tensors="pt").to(self.device)
                        with torch.no_grad():
                            image_features = self.clip_model.get_image_features(**image_input)
                            image_features /= image_features.norm(dim=-1, keepdim=True)
                        
                        object_vector = image_features.cpu().numpy()
                        self.object_vector_cache.set(cache_key, object_vector)
                    except Exception as e:
                        print(f"Lá»—i khi xá»­ lÃ½ áº£nh crop cho {keyframe_id}: {e}")
                        continue
                
                # TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng
                image_tensor = torch.from_numpy(object_vector).to(self.device)
                similarity = util.pytorch_cos_sim(image_tensor, text_features[i].unsqueeze(0))
                total_score += similarity.item()

            cand['scores']['fine_grained_score'] = total_score / len(verification_rules) if verification_rules else 1.0

        # GÃ¡n Ä‘iá»ƒm máº·c Ä‘á»‹nh cho cÃ¡c á»©ng viÃªn khÃ´ng Ä‘Æ°á»£c check
        for cand in candidates[50:]:
            cand['scores']['fine_grained_score'] = 0.5 # Äiá»ƒm trung bÃ¬nh

        return candidates

    def search(self,
               query_text: str,
               top_k_final: int,
               top_k_retrieval: int,
               precomputed_analysis: Dict[str, Any] = None,
               weights: Dict[str, float] = None
              ) -> List[Dict[str, Any]]:
        """
        Thá»±c hiá»‡n tÃ¬m kiáº¿m vÃ  tÃ¡i xáº¿p háº¡ng Ä‘a táº§ng theo kiáº¿n trÃºc PHOENIX.
        Luá»“ng xá»­ lÃ½: Contextual -> Spatial -> Fine-grained Verification.
        """
        print("\n--- ğŸ”± Báº¯t Ä‘áº§u quy trÃ¬nh tÃ¬m kiáº¿m Ä‘a táº§ng PHOENIX... ---")

        # --- BÆ°á»›c 0: Chuáº©n bá»‹ ---
        if precomputed_analysis is None: precomputed_analysis = {}
        # Äáº·t trá»ng sá»‘ máº·c Ä‘á»‹nh vÃ  cho phÃ©p ghi Ä‘Ã¨ tá»« UI
        final_weights = {
            'w_clip': 0.2, 
            'w_semantic': 0.3, 
            'w_spatial': 0.25, 
            'w_fine_grained': 0.25, 
            **(weights or {})
        }
        print(f"    -> Trá»ng sá»‘ há»a lá»±c: {final_weights}")

        # --- Táº¦NG 1: Bá»˜ Lá»ŒC NGá»® Cáº¢NH (Láº¥y á»©ng viÃªn thÃ´ báº±ng CLIP toÃ n cá»¥c) ---
        print(f"--- Táº§ng 1: Láº¥y Top-{top_k_retrieval} á»©ng viÃªn theo Ngá»¯ cáº£nh... ---")
        candidates = self.basic_searcher.search(query_text, top_k=top_k_retrieval)
        if not candidates:
            print("--- â›” KhÃ´ng tÃ¬m tháº¥y á»©ng viÃªn nÃ o á»Ÿ Táº§ng 1. Dá»«ng tÃ¬m kiáº¿m. ---")
            return []
        print(f"    -> TÃ¬m tháº¥y {len(candidates)} á»©ng viÃªn tiá»m nÄƒng.")
        
        # Khá»Ÿi táº¡o cáº¥u trÃºc Ä‘iá»ƒm cho má»—i á»©ng viÃªn
        for cand in candidates:
            cand['scores'] = {'clip_score': cand.get('clip_score', 0.0)}

        # --- RERANKING NGá»® NGHÄ¨A (Tinh chá»‰nh Ä‘iá»ƒm ngá»¯ cáº£nh báº±ng Bi-Encoder) ---
        # (Pháº§n nÃ y báº¡n cáº§n Ä‘áº£m báº£o logic rerank_batch cá»§a mÃ¬nh Ä‘Æ°á»£c tÃ­ch há»£p á»Ÿ Ä‘Ã¢y)
        # Giáº£ sá»­ sau bÆ°á»›c nÃ y, 'semantic_score' Ä‘Æ°á»£c thÃªm vÃ o
        print("--- Táº§ng 1.5: Tinh chá»‰nh Ä‘iá»ƒm Ngá»¯ nghÄ©a báº±ng Bi-Encoder... ---")
        # VÃ­ dá»¥:
        # candidates = self.rerank_with_bi_encoder(candidates, query_text)
        # Táº¡m thá»i gÃ¡n Ä‘iá»ƒm giáº£ Ä‘á»‹nh Ä‘á»ƒ code cháº¡y Ä‘Æ°á»£c
        for cand in candidates:
             cand['scores']['semantic_score'] = cand['scores']['clip_score'] # Táº¡m thá»i gÃ¡n báº±ng Ä‘iá»ƒm clip
        print("    -> HoÃ n táº¥t tinh chá»‰nh Ä‘iá»ƒm ngá»¯ nghÄ©a.")


        # --- Táº¦NG 2: Bá»˜ Lá»ŒC QUAN Há»† KHÃ”NG GIAN ---
        spatial_rules = precomputed_analysis.get('spatial_rules', [])
        grounded_entities = precomputed_analysis.get('grounded_entities', []) # CÃ³ thá»ƒ dÃ¹ng trong tÆ°Æ¡ng lai
        
        candidates_after_spatial = self._apply_spatial_filter(candidates, spatial_rules, grounded_entities)


        # --- Táº¦NG 3: Bá»˜ Lá»ŒC XÃC THá»°C CHI TIáº¾T ---
        verification_rules = precomputed_analysis.get('fine_grained_verification', [])
        
        # Sáº¯p xáº¿p láº¡i trÆ°á»›c khi Ä‘Æ°a vÃ o Táº§ng 3 Ä‘á»ƒ Ä‘áº£m báº£o chá»‰ "soi" nhá»¯ng á»©ng viÃªn tá»‘t nháº¥t
        # TÃ­nh Ä‘iá»ƒm táº¡m thá»i sau Táº§ng 2
        for cand in candidates_after_spatial:
            s = cand['scores']
            cand['temp_score'] = (
                final_weights['w_clip'] * s.get('clip_score', 0.0) +
                final_weights['w_semantic'] * s.get('semantic_score', 0.0) +
                final_weights['w_spatial'] * s.get('spatial_score', 0.5)
            )
        
        sorted_before_fine_grained = sorted(candidates_after_spatial, key=lambda x: x.get('temp_score', 0.0), reverse=True)

        candidates_after_fine_grained = self._apply_fine_grained_filter(sorted_before_fine_grained, verification_rules)


        # --- BÆ¯á»šC CUá»I: TÃNH ÄIá»‚M Tá»”NG Há»¢P VÃ€ Sáº®P Xáº¾P ---
        print("--- ğŸ¯ TÃ­nh toÃ¡n Ä‘iá»ƒm há»a lá»±c cuá»‘i cÃ¹ng vÃ  sáº¯p xáº¿p... ---")
        for cand in candidates_after_fine_grained:
            scores = cand['scores']
            
            # CÃ´ng thá»©c Ä‘iá»ƒm hoÃ n chá»‰nh
            final_score = (
                final_weights['w_clip'] * scores.get('clip_score', 0.0) +
                final_weights['w_semantic'] * scores.get('semantic_score', 0.0) +
                final_weights['w_spatial'] * scores.get('spatial_score', 0.5) + # DÃ¹ng 0.5 lÃ m Ä‘iá»ƒm máº·c Ä‘á»‹nh náº¿u cÃ³ lá»—i
                final_weights['w_fine_grained'] * scores.get('fine_grained_score', 0.5) # DÃ¹ng 0.5 lÃ m Ä‘iá»ƒm máº·c Ä‘á»‹nh
            )
            cand['final_score'] = final_score

        # Sáº¯p xáº¿p láº¡i láº§n cuá»‘i cÃ¹ng dá»±a trÃªn Ä‘iá»ƒm tá»•ng há»£p
        final_sorted_candidates = sorted(candidates_after_fine_grained, key=lambda x: x.get('final_score', 0.0), reverse=True)
        
        print(f"--- âœ… Quy trÃ¬nh PHOENIX hoÃ n táº¥t. Tráº£ vá» Top-{top_k_final} káº¿t quáº£. ---")
        
        # In ra 3 káº¿t quáº£ Ä‘áº§u tiÃªn Ä‘á»ƒ debug
        for i, cand in enumerate(final_sorted_candidates[:3]):
            print(f"  Top {i+1}: {cand['keyframe_id']} | Score: {cand['final_score']:.4f} | Scores: {cand['scores']}")

        return final_sorted_candidates[:top_k_final]